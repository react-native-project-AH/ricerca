{"ast":null,"code":"import _slicedToArray from \"@babel/runtime/helpers/slicedToArray\";\nvar _jsxFileName = \"C:\\\\Users\\\\Home\\\\Desktop\\\\ricerca\\\\components\\\\search\\\\index.js\";\nimport React, { useState, useEffect, useContext } from 'react';\nimport Text from \"react-native-web/dist/exports/Text\";\nimport StyleSheet from \"react-native-web/dist/exports/StyleSheet\";\nimport View from \"react-native-web/dist/exports/View\";\nimport TextInput from \"react-native-web/dist/exports/TextInput\";\nimport Button from \"react-native-web/dist/exports/Button\";\nimport SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';\nimport { Icon } from 'react-native-elements';\nimport { PrivateContext } from \"../../context/valid\";\n\nfunction Search(props) {\n  var _React$useState = React.useState(),\n      _React$useState2 = _slicedToArray(_React$useState, 2),\n      recording = _React$useState2[0],\n      setRecording = _React$useState2[1];\n\n  var _useSpeechRecognition = useSpeechRecognition(),\n      transcript = _useSpeechRecognition.transcript,\n      resetTranscript = _useSpeechRecognition.resetTranscript;\n\n  var validation = useContext(PrivateContext);\n  useEffect(function () {\n    var a = document.querySelector('#gsc-i-id1');\n\n    if (a) {\n      a.value = transcript;\n      a.style.background = 'none';\n      secretPath();\n    }\n  }, [transcript]);\n  useEffect(function () {\n    var cx = '4050613ba5d231066';\n    var gcse = document.createElement('script');\n    gcse.type = 'text/javascript';\n    gcse.async = true;\n    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;\n    var s = document.getElementsByTagName('script')[0];\n    s.parentNode.insertBefore(gcse, s);\n  }, []);\n\n  var secretPath = function secretPath() {\n    var _document$querySelect;\n\n    var secret = (_document$querySelect = document.querySelector('#gsc-i-id1')) == null ? void 0 : _document$querySelect.value;\n\n    if (secret.toUpperCase() !== \"JORDAN\") {\n      document.querySelector('.gsc-search-button-v2').click();\n    } else {\n      validation.setValid(true);\n    }\n  };\n\n  if (!SpeechRecognition.browserSupportsSpeechRecognition()) {\n    return null;\n  }\n\n  return React.createElement(View, {\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 80,\n      columnNumber: 9\n    }\n  }, React.createElement(\"div\", {\n    className: \"gcse-search\",\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 82,\n      columnNumber: 1\n    }\n  }), React.createElement(Icon, {\n    name: \"g-translate\",\n    onPress: function onPress() {\n      SpeechRecognition.startListening({\n        continuous: true,\n        language: 'ar-JO'\n      });\n    },\n    __self: this,\n    __source: {\n      fileName: _jsxFileName,\n      lineNumber: 84,\n      columnNumber: 1\n    }\n  }));\n}\n\nexport default Search;","map":{"version":3,"sources":["C:/Users/Home/Desktop/ricerca/components/search/index.js"],"names":["React","useState","useEffect","useContext","SpeechRecognition","useSpeechRecognition","Icon","PrivateContext","Search","props","recording","setRecording","transcript","resetTranscript","validation","a","document","querySelector","value","style","background","secretPath","cx","gcse","createElement","type","async","src","s","getElementsByTagName","parentNode","insertBefore","secret","toUpperCase","click","setValid","browserSupportsSpeechRecognition","startListening","continuous","language"],"mappings":";;AAAA,OAAOA,KAAP,IAAgBC,QAAhB,EAA0BC,SAA1B,EAAqCC,UAArC,QAAuD,OAAvD;;;;;;AAEA,OAAOC,iBAAP,IAA4BC,oBAA5B,QAAwD,0BAAxD;AACA,SAASC,IAAT,QAAqB,uBAArB;AACA,SAAQC,cAAR;;AAKA,SAASC,MAAT,CAAgBC,KAAhB,EAAuB;AACnB,wBAAkCT,KAAK,CAACC,QAAN,EAAlC;AAAA;AAAA,MAAOS,SAAP;AAAA,MAAkBC,YAAlB;;AACA,8BAAwCN,oBAAoB,EAA5D;AAAA,MAAQO,UAAR,yBAAQA,UAAR;AAAA,MAAoBC,eAApB,yBAAoBA,eAApB;;AAIA,MAAMC,UAAU,GAAGX,UAAU,CAACI,cAAD,CAA7B;AAEAL,EAAAA,SAAS,CAAC,YAAI;AACV,QAAIa,CAAC,GAAGC,QAAQ,CAACC,aAAT,CAAuB,YAAvB,CAAR;;AACA,QAAGF,CAAH,EAAK;AACDA,MAAAA,CAAC,CAACG,KAAF,GAAUN,UAAV;AACAG,MAAAA,CAAC,CAACI,KAAF,CAAQC,UAAR,GAAqB,MAArB;AACAC,MAAAA,UAAU;AACb;AACJ,GAPQ,EAOP,CAACT,UAAD,CAPO,CAAT;AAQAV,EAAAA,SAAS,CAAC,YAAI;AACJ,QAAIoB,EAAE,GAAG,mBAAT;AACA,QAAIC,IAAI,GAAGP,QAAQ,CAACQ,aAAT,CAAuB,QAAvB,CAAX;AACAD,IAAAA,IAAI,CAACE,IAAL,GAAY,iBAAZ;AACAF,IAAAA,IAAI,CAACG,KAAL,GAAa,IAAb;AACAH,IAAAA,IAAI,CAACI,GAAL,GAAW,sCAAsCL,EAAjD;AACA,QAAIM,CAAC,GAAGZ,QAAQ,CAACa,oBAAT,CAA8B,QAA9B,EAAwC,CAAxC,CAAR;AACAD,IAAAA,CAAC,CAACE,UAAF,CAAaC,YAAb,CAA0BR,IAA1B,EAAgCK,CAAhC;AAET,GATQ,EASP,EATO,CAAT;;AAWA,MAAMP,UAAU,GAAG,SAAbA,UAAa,GAAI;AAAA;;AACnB,QAAIW,MAAM,4BAAGhB,QAAQ,CAACC,aAAT,CAAuB,YAAvB,CAAH,qBAAG,sBAAsCC,KAAnD;;AACA,QAAGc,MAAM,CAACC,WAAP,OAAyB,QAA5B,EAAqC;AACjCjB,MAAAA,QAAQ,CAACC,aAAT,CAAuB,uBAAvB,EAAgDiB,KAAhD;AACH,KAFD,MAEM;AACFpB,MAAAA,UAAU,CAACqB,QAAX,CAAoB,IAApB;AACH;AACJ,GAPD;;AAmCA,MAAI,CAAC/B,iBAAiB,CAACgC,gCAAlB,EAAL,EAA2D;AACvD,WAAO,IAAP;AACH;;AAID,SAEI,oBAAC,IAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,KAER;AAAK,IAAA,SAAS,EAAC,aAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAFQ,EAIR,oBAAC,IAAD;AAAM,IAAA,IAAI,EAAC,aAAX;AAAyB,IAAA,OAAO,EAAE,mBAAI;AAAChC,MAAAA,iBAAiB,CAACiC,cAAlB,CAAiC;AACpEC,QAAAA,UAAU,EAAE,IADwD;AAEpEC,QAAAA,QAAQ,EAAE;AAF0D,OAAjC;AAGpC,KAHH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAJQ,CAFJ;AAsCH;;AACD,eAAe/B,MAAf","sourcesContent":["import React, { useState, useEffect, useContext } from 'react';\r\nimport { Text, StyleSheet, View, TextInput, Button } from 'react-native';\r\nimport SpeechRecognition, { useSpeechRecognition } from 'react-speech-recognition';\r\nimport { Icon } from 'react-native-elements';\r\nimport {PrivateContext} from '../../context/valid';\r\n\r\n// import { Audio } from 'expo-av';\r\n// AIzaSyAvcNX1GBJilOOjrs-Ch-CG9VX8yG-nRDk\r\n\r\nfunction Search(props) {\r\n    const [recording, setRecording] = React.useState();    \r\n    const { transcript, resetTranscript } = useSpeechRecognition()\r\n\r\n\r\n\r\n    const validation = useContext(PrivateContext);\r\n\r\n    useEffect(()=>{\r\n        let a = document.querySelector('#gsc-i-id1');\r\n        if(a){\r\n            a.value = transcript;\r\n            a.style.background = 'none';\r\n            secretPath()\r\n        } \r\n    },[transcript])\r\n    useEffect(()=>{\r\n              var cx = '4050613ba5d231066';\r\n              var gcse = document.createElement('script');\r\n              gcse.type = 'text/javascript';\r\n              gcse.async = true;\r\n              gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;\r\n              var s = document.getElementsByTagName('script')[0];\r\n              s.parentNode.insertBefore(gcse, s);\r\n\r\n    },[])\r\n\r\n    const secretPath = ()=>{\r\n        let secret = document.querySelector('#gsc-i-id1')?.value;\r\n        if(secret.toUpperCase() !== \"JORDAN\"){\r\n            document.querySelector('.gsc-search-button-v2').click();\r\n        }else {\r\n            validation.setValid(true)\r\n        }\r\n    }\r\n    // const startRecording = async ()=> {\r\n    //     try {\r\n    //         console.log('Requesting permissions..');\r\n    //         await Audio.requestPermissionsAsync()\r\n    //         await Audio.setAudioModeAsync({\r\n    //             allowsRecordingIOS: true,\r\n    //             playsInSilentModeIOS: true,\r\n    //       }); \r\n    //       console.log('Starting recording..');\r\n    //       const recording = new Audio.Recording();\r\n    //       await recording.prepareToRecordAsync(Audio.RECORDING_OPTIONS_PRESET_HIGH_QUALITY);\r\n    //       await recording.startAsync(); \r\n    //       setRecording(recording);\r\n    //       console.log('Recording started');\r\n    //     } catch (err) {\r\n    //         console.error('Failed to start recording', err);\r\n    //     }\r\n    // }\r\n    \r\n    // async function stopRecording() {\r\n    //     console.log('Stopping recording..');\r\n    //     setRecording(undefined);\r\n    //     await recording.stopAndUnloadAsync();\r\n    //     const uri = recording.getURI(); \r\n    //     console.log('Recording stopped and stored at', uri);\r\n    // }\r\n    \r\n    if (!SpeechRecognition.browserSupportsSpeechRecognition()) {\r\n        return null\r\n    }\r\n\r\n\r\n    \r\n    return (\r\n\r\n        <View>\r\n            \r\n<div className=\"gcse-search\"></div>\r\n\r\n<Icon name='g-translate' onPress={()=>{SpeechRecognition.startListening({\r\n    continuous: true,\r\n    language: 'ar-JO'\r\n})}}/>\r\n\r\n            {/* <Formik initialValues={{ search: \"\" }}\r\n                onSubmit={(values) => {\r\n                    console.log(values)\r\n                }}\r\n                >\r\n                {formikProps => (\r\n                    <Form>\r\n                        <TextInput\r\n                            placeholder=\"SEARCH\"\r\n                            onChangeText={formikProps.handleChange(\"search\")}\r\n                        />\r\n                        <Button title=\"submit\" onPress={formikProps.handleSubmit} />\r\n                    </Form>\r\n                )}\r\n\r\n            </Formik> */}\r\n            {/* <div>\r\n               \r\n                <button onClick={SpeechRecognition.stopListening}>Stop</button>\r\n                <button onClick={resetTranscript}>Reset</button>\r\n            </div> */}\r\n            {/* <Button\r\n        title={recording ? 'Stop Recording' : 'Start Recording'}\r\n        onPress={recording ? stopRecording : startRecording}\r\n      /> */}\r\n        </View>\r\n    )\r\n}\r\nexport default Search;"]},"metadata":{},"sourceType":"module"}